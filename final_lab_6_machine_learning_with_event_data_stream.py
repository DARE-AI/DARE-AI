# -*- coding: utf-8 -*-
"""Final Lab 6: Machine Learning with event data stream

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GCfqYg7I86Z98F30NXw1U-6cM_apBPjg

# Lab 6: Online Learning using Apache Kafka
#### Using Apache Kafka to simulate real time data stream model training.

### Learning Objecives
* Learn to import kafka and write to and read from kafka.
* Learn to use layers in keras.
* Learn to use kafka for data streams and train the model in real-time.
* Learn the concept of online learning.

### Install Package
Install Kafka and Tensorflow IO. Tensorflow IO is an extension package to Tensorflow which supports integration with Apache Kafka and other systems.
"""

# !pip install tensorflow-io
!pip install kafka-python

!pip install tensorflow_io==0.23.1

"""### Imports
Import all the required libraries for the lab including Kafka, pandas, Tensorflow, and more.


"""

import time 
from kafka import KafkaProducer
from sklearn.model_selection import train_test_split
import pandas as pd 
import tensorflow as tf
import tensorflow_io as tfio

"""### Download Kafka
Download and setup Kafka for real time data stream simulation. 


"""

!curl -sSOL https://downloads.apache.org/kafka/2.8.1/kafka_2.12-2.8.1.tgz
!tar -xzf kafka_2.12-2.8.1.tgz

"""### Start running Kafka and Zookeeper server instances 

Start Kafka and Zookeeper servers as a daemon processes. Zookeeper is a centralized service for maintaing configuration information, naming, providing distributed synchronization, and providing group services.
"""

!./kafka_2.12-2.8.1/bin/zookeeper-server-start.sh -daemon ./kafka_2.12-2.8.1/config/zookeeper.properties
!./kafka_2.12-2.8.1/bin/kafka-server-start.sh -daemon ./kafka_2.12-2.8.1/config/server.properties

"""### Create a topic to store events
Create topic for train and test dataset to store events in Kafka. Kafka is a distributed event streaming platform that lets you read, write, store and process events. These events or messages are organized and stored in topics. In simple terms, topic is similar to a folder in a filesystem, and the events are the file in that folder.
"""

!./kafka_2.12-2.8.1/bin/kafka-topics.sh --create --topic susy-train --bootstrap-server localhost:9092 --replication-factor 1 --partitions 1
!./kafka_2.12-2.8.1/bin/kafka-topics.sh --create --topic susy-test --bootstrap-server localhost:9092 --replication-factor 1 --partitions 2

"""### Describe the topic for details
Describe command helps us gather details on topic, it's partitions, replicas, and other important information.

"""

!./kafka_2.12-2.8.1/bin/kafka-topics.sh --describe --topic susy-train --bootstrap-server localhost:9092
!./kafka_2.12-2.8.1/bin/kafka-topics.sh --describe --topic susy-test --bootstrap-server localhost:9092

"""### Mount Google Drive

In the code cell below, we mount the google drive to the colab environment so that we have access to the local version of the dataset.
"""

from google.colab import drive
drive.mount('/content/drive')

"""### Define features
COLUMNS is used to define each of the feature in the SUSY dataset.
"""

COLUMNS = [
           'class',
           'lepton_1_pT',
           'lepton_1_eta',
           'lepton_1_phi',
           'lepton_2_pT',
           'lepton_2_eta',
           'lepton_2_phi',
           'missing_energy_magnitude',
           'missing_energy_phi',
           'MET_rel',
           'axial_MET',
           'M_R',
           'M_TR_2',
           'R',
           'MT2',
           'S_R',
           'M_Delta_R',
           'dPhi_r_b',
           'cos(theta_r1)']

"""### SUSY Dataset

SUSY Data set is produced using Monte Carlo simulations. It is the data produced from the particle accelerators. The first column of the dataset is the label followed by 8 features which are kinematic properties measured by the particle detectors in the accelerator. The last 10 features are the high-level features derived by physicists to help discriminate between the two classes signal process or a background process.

### Read CSV
Use Pandas to load SUSY dataset from the CSV file and provide the column name for each of the features.
"""

mydata = pd.read_csv('/content/drive/My Drive/Intro2MLDatasets/Lab6/SUSY.csv', header=None, names=COLUMNS, nrows=100000)

"""### Visualize Data

Panda functions helps us visualize the SUSY dataset. 
"""

mydata.head()

"""### Split Train and Test data
As always, it is necessary to split the data into train, test, and validation. In this context, we are splitting 80% of the data to be the train data. The remaining 20% of the data is split between test and validation dataset.
"""

train_df, test_df = train_test_split(mydata, test_size=0.3, shuffle=True)
print("Number of training samples: ",len(train_df))
print("Number of testing sample: ",len(test_df))

x_train_df = train_df.drop(["class"], axis=1)
y_train_df = train_df["class"]

x_test_df = test_df.drop(["class"], axis=1)
y_test_df = test_df["class"]

"""### Convert data to list format
Read each row from the dataframe and convert it to the list format to feed to Kafka.
"""

#Convert each test and train dataframe to list form to feed to kafka
x_train = list(filter(None, x_train_df.to_csv(index=False).split("\n")[1:]))
y_train = list(filter(None, y_train_df.to_csv(index=False).split("\n")[1:]))

x_test = list(filter(None, x_test_df.to_csv(index=False).split("\n")[1:]))
y_test = list(filter(None, y_test_df.to_csv(index=False).split("\n")[1:]))
len(x_train), len(y_train), len(x_test), len(y_test)

NUM_COLUMNS = len(x_train_df.columns)

print(NUM_COLUMNS)

"""### Create Kafka Producer 
Create Kafka producer which takes in data and sends the record to the partition within a topic in Kafka cluster. 
"""

#send each record to a partition within a topic in kafka cluster
def write_to_kafka(topic, items):
  count=0
  producer = KafkaProducer(bootstrap_servers=['localhost:9092'])
  for message, key in items:
    producer.send(topic, key=key.encode('utf-8'), value=message.encode('utf-8'))
    count += 1 
  producer.flush()
  print("Wrote {0} messages into topic: {1}".format(count, topic))

write_to_kafka("susy-train", zip(x_train, y_train))
write_to_kafka("susy-test", zip(x_test, y_test))

"""### Online Learning
Unlike traditional training of machine learning models, online learning is based on incrementally learning or updating parameters as soon as the new data points are available. This process continues indefinitely. In the code below, stream_timeout is set to 10000 milliseconds which means as all the messages are consumed from the topic, the dataset will wait for 10 more seconds before timing out and disconnecting from the Kafka cluster. If additional data arrives in that time period, model training resumes. 
"""

online_train_ds = tfio.experimental.streaming.KafkaGroupIODataset(
    topics=["susy-train"],
    group_id="cgonline",
    servers="localhost:9092",
    stream_timeout=10000, # in milliseconds, to block indefinitely, set it to -1.
    configuration=[
        "session.timeout.ms=7000",
        "max.poll.interval.ms=8000",
        "auto.offset.reset=earliest"
    ],
)

def decode_kafka_online_item(raw_message, raw_key):
  message = tf.io.decode_csv(raw_message, [[0.0] for i in range(NUM_COLUMNS)])
  key = tf.strings.to_number(raw_key)
  return (message, key)

BATCH_SIZE = 32
online_train_ds = online_train_ds.shuffle(buffer_size=32)
online_train_ds = online_train_ds.map(decode_kafka_online_item)
online_train_ds = online_train_ds.batch(BATCH_SIZE)

"""### Initialize variables to create ANN
Initialize Optimizer, loss function, metrics function, and epochs for Neural Network.

"""

OPTIMIZER="adam"
LOSS = tf.keras.losses.BinaryCrossentropy()
METRICS = ['accuracy']
EPOCHS = 1

"""### Define Model
Create Neural network containing multiple layers and use Dropout layer to prevent overfitting in model. 
"""

#define model input shape, and layers of NN
model = tf.keras.Sequential()
model.add(tf.keras.layers.Dense(128, input_shape=(NUM_COLUMNS,), activation='relu'))
model.add(tf.keras.layers.Dropout(0.2))
model.add(tf.keras.layers.Dense(256, activation='relu'))
model.add(tf.keras.layers.Dropout(0.4))
model.add(tf.keras.layers.Dense(128, activation='relu'))
model.add(tf.keras.layers.Dropout(0.4))
model.add(tf.keras.layers.Dense(1, activation='sigmoid'))
print(model.summary())

model.compile(optimizer=OPTIMIZER, loss=LOSS, metrics=METRICS)

model.fit(online_train_ds, epochs=EPOCHS)

"""### Prepare test data

Prepare the test dataset using KafkaGroupIODataset to stream and test with the model we initialized before.
"""

#prepare test data to evaluate on the online trained model
test_ds = tfio.experimental.streaming.KafkaGroupIODataset(
    topics=["susy-test"],
    group_id="testcg",
    servers="localhost:9092",
    stream_timeout=10000,
    configuration=[
        "session.timeout.ms=7000",
        "max.poll.interval.ms=8000",
        "auto.offset.reset=earliest"
    ],
)

def decode_kafka_test_item(raw_message, raw_key):
  message = tf.io.decode_csv(raw_message, [[0.0] for i in range(NUM_COLUMNS)])
  key = tf.strings.to_number(raw_key)
  return (message, key)

test_ds = test_ds.map(decode_kafka_test_item)
test_ds = test_ds.batch(32)

"""### Evaluate Model
Use predefined function evaluate to figure loss value and metric value with the test data.
"""

res = model.evaluate(test_ds)

!./kafka_2.12-2.8.1/bin/kafka-consumer-groups.sh --bootstrap-server localhost:9092 --describe --group cgonline
!./kafka_2.12-2.8.1/bin/kafka-consumer-groups.sh --bootstrap-server localhost:9092 --describe --group testcg

"""**What is the difference between this Lab (Lab 6) and Lab 4 (IDS)? [5 points]**

**Did you observe any differences in result during the evlaution of the model when you rerun? [5 points]**
"""